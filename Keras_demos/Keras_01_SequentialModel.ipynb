{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 1. Intro to sequential models API\n",
    "<hr>\n",
    "## 1.1. Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequential model instantiation using class constructor\n",
    "model = Sequential(layers=[Dense(32, input_shape=(784,)), Activation('relu'), Dense(10), Activation('softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as before but explicit\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 1.2. Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model compilation\n",
    "\n",
    "#multi-class classification problem\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#binary classification problem\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#regression problem\n",
    "model.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Custom metrics ans loss functions\n",
    "Some problems require custom metrics and/or loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def myloss(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(y_true - y_pred)**2)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=myloss , metrics=[mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 1.3. Model training\n",
    "## 1.3.1. Binary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#generate dummy data with scikit learn\n",
    "#import numpy as np\n",
    "#data = np.random.random((1000,100))\n",
    "#labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "dataset = make_classification(n_samples = 1000, n_features = 100, n_classes = 2, class_sep=0.9)\n",
    "data = dataset[0]\n",
    "labels = dataset[1]\n",
    "\n",
    "#train the model iterating on the data on batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 1.3.2. Multiclass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#generate dummy data with scikit learn\n",
    "dataset = make_classification(n_samples = 1000, n_features = 100, n_informative=5, n_classes = 10, class_sep=0.9, n_clusters_per_class=1)\n",
    "data = dataset[0]\n",
    "labels = dataset[1]\n",
    "\n",
    "#convert labels to one-hot-encoding representation\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "#train\n",
    "model.fit(data, one_hot_labels, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 2. Sequential complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>\n",
    "## 2.1. Multilayer Perceptron for multi-class softmax classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "dataset = make_classification(n_samples = 1000, n_features = 20, n_informative=10, n_classes = 3, class_sep=0.98, n_clusters_per_class=1)\n",
    "data = dataset[0]\n",
    "labels = dataset[1]\n",
    "\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=3)\n",
    "\n",
    "#train and test datasets 70-30\n",
    "random_index = np.random.permutation(1000)\n",
    "\n",
    "train_data = data[random_index[range(900)]]\n",
    "train_labels = one_hot_labels[random_index[range(900)]]\n",
    "test_data = data[random_index[range(900,1000)]]\n",
    "test_labels = one_hot_labels[random_index[range(900, 1000)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, epochs = 100, batch_size=128)\n",
    "print(\"Train finished\")\n",
    "\n",
    "score = model.evaluate(test_data, test_labels, batch_size=128)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Multilayer perceptron for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET\n",
    "\n",
    "dataset = make_classification(n_samples = 1000, n_features = 20, n_informative=2, n_classes = 2, class_sep=0.98, n_clusters_per_class=1)\n",
    "data = dataset[0]\n",
    "labels = dataset[1]\n",
    "\n",
    "#train and test datasets 70-30\n",
    "random_index = np.random.permutation(1000)\n",
    "\n",
    "train_data = data[random_index[range(900)]]\n",
    "train_labels = labels[random_index[range(900)]]\n",
    "test_data = data[random_index[range(900,1000)]]\n",
    "test_labels = labels[random_index[range(900, 1000)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, momentum=0.9, decay=1e-6)\n",
    "\n",
    "model.compile(optimizer=sgd, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=128)\n",
    "\n",
    "print(model.evaluate(test_data, test_labels, batch_size=128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2.3. Convolutional network (like VGG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR / MNIST dataset example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import mnist\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_index = np.random.permutation(1000)\n",
    "test_index = np.random.permutation(100)\n",
    "\n",
    "x_train = x_train.astype('float32')[train_index]\n",
    "x_test = x_test.astype('float32')[test_index]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = utils.to_categorical(y_train, num_classes)[train_index]\n",
    "y_test = utils.to_categorical(y_test, num_classes)[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=x_train.shape[1:], padding='same'))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2.4. Sequence classification with LSTM ==> case: imdb sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "#load imdb dataset (sentiment analysis)\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences ==> [sequence, output]\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=15, batch_size=batch_size, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model_json = model.to_json()\n",
    "with open(\"imdb_sentiment_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#save weights\n",
    "model.save_weights(\"imdb_sentiment_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16115\n"
     ]
    }
   ],
   "source": [
    "print word_to_id['spiders']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2.5. Sequence classification with 1D Convolution ==> case: imdb sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load imdb dataset (sentiment analysis)\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences ==> [sequence, output]\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#embedding layer\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "#convolutional layer\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "#MLP layer\n",
    "model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#Sigmoid layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2.6. Sequence classification with stacked LSTM==> case: imdb sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000, 'train sequences')\n",
      "(25000, 'test sequences')\n",
      "Pad sequences (samples x time)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "#load imdb dataset (sentiment analysis)\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "#pad sequences ==> [sequence, output]\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 140s 6ms/step - loss: 0.4340 - acc: 0.7922 - val_loss: 0.3536 - val_acc: 0.8439\n",
      "142.674567938\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=batch_size, validation_data=(x_test, y_test))\n",
    "end = time.time()\n",
    "\n",
    "print(end-time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2.7. Sequence classification with bidirectional LSTM==> case: imdb sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 105s 4ms/step - loss: 0.4220 - acc: 0.8022 - val_loss: 0.3587 - val_acc: 0.8412\n",
      "106.84355092\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=batch_size, validation_data=(x_test, y_test))\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
