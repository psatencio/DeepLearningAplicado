{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/itm_logo.jpg\" width=\"300px\">\n",
    "\n",
    "## Inteligencia Artificial - IAI84\n",
    "### Instituto Tecnológico Metropolitano\n",
    "#### Pedro Atencio Ortiz - 2018\n",
    "\n",
    "\n",
    "En este notebook se implementa un clasificador binario mediante regresion logistica. La regresion logistica constituye el elemento basico de las redes neuronales. En otras palabras, podriamos asumir que un regresor logistico constituye una neurona en una red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 1. Propagación hacia adelante (forward propagation)\n",
    "\n",
    "La primera etapa de una regresion logistica consiste en calcular la salida del modelo dada una entrada. Este proceso se conoce como propagacion hacia adelante. Esta propagacion se calcula de la siguiente manera:\n",
    "\n",
    "$a = \\sigma(W^{T}X+b)$\n",
    "\n",
    "Donde $\\sigma$ es la funcion sigmoide, $W$ son los pesos de las conexiones del regresor, $b$ es el bias y $x$ la entrada.\n",
    "\n",
    "La funcion $\\sigma$ se define como:\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Primero definamos la activacion lineal definida como: $W^{T}x+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation(W, b, X):\n",
    "    z = np.dot(W.T,X) + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3],[4,5,6]]).T\n",
    "print(\"X: \",X)\n",
    "\n",
    "Y = np.array([[0, 1]])\n",
    "print(\"Y: \", Y)\n",
    "\n",
    "W = np.array([[0.4], [-0.5], [0.01]])\n",
    "print(\"W: \", W)\n",
    "\n",
    "b = 0.3\n",
    "print(\"b: \", b)\n",
    "\n",
    "A = linear_activation(W, b, X)\n",
    "\n",
    "print(\"Activacion Lineal: \", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "Definamos ahora la funcion sigmoide $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Returns sigmoid activation for array z\n",
    "    '''\n",
    "    a = 1. / (1. + np.exp(-z)) \n",
    "    \n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5,5,20) #x entre -5 y 5\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sigmoid(x)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Una vez tenemos la activacion lineal y la funcion sigmoide, entonces podemos realizar la propagacion hacia adelante:\n",
    "\n",
    "$a = \\sigma(W^{T}X+b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3],[4,5,6]]).T\n",
    "W = np.array([[0.4], [-0.5], [0.01]])\n",
    "b = 0.3\n",
    "\n",
    "A = sigmoid(linear_activation(W, b, X))\n",
    "\n",
    "print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 2. Función de perdida\n",
    "\n",
    "Esta funcion permite medir la calidad del entrenamiento del regresor logistico. A menor valor, mejor desempenio. Existen muchas funciones de perdida, en este caso utilizamos una funcion muy conocida, denominada perdida logaritmica o logloss.\n",
    "\n",
    "$L(Y,A) = -\\left ( Ylog(A)+(1-Y)log(1-A) \\right )$\n",
    "\n",
    "Donde $Y$ y $A$ son la salida de referencia y la salida de la red respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(Y, A):\n",
    "    return -(Y * np.log(A) + (1-Y) * np.log(1-A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 2 #to be able to verify your result\n",
    "np.random.seed(seed)\n",
    "W = np.random.randn(2,1)\n",
    "b = np.random.rand()\n",
    "X = np.random.randn(2, 3)\n",
    "\n",
    "Y = np.array([[1,1,0]]) #original labels for features X\n",
    "A = sigmoid(linear_activation(W,b,X)) #forward activation\n",
    "\n",
    "print(\"Salida de referencia o esperada: \", Y)\n",
    "print(\"Salida del regresor logistico: \", A)\n",
    "\n",
    "print(\"Perdida dato a dato: \", loss(Y, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 3. Función de costo $J$\n",
    "\n",
    "La funcion de perdida mide el error del regresor dato a dato. El costo mide el error total. Para este caso midamos el costo $J$ como el promedio de las perdidas dato a dato.\n",
    "\n",
    "$J(L) = \\frac{\\sum{L}}{n}$\n",
    "\n",
    "Donde $n$ es el numero de datos que se tengan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(L):\n",
    "    return np.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 2 #to be able to verify your result\n",
    "np.random.seed(seed)\n",
    "W = np.random.randn(2,1)\n",
    "b = np.random.rand()\n",
    "X = np.random.randn(2, 3)\n",
    "\n",
    "Y = np.array([[1,1,0]]) #original labels for features X\n",
    "A = sigmoid(linear_activation(W,b,X)) #forward activation\n",
    "\n",
    "L = loss(Y,A)\n",
    "J = cost(L)\n",
    "\n",
    "print(\"Perdida: \", L)\n",
    "print(\"Costo: \", J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 4. Descenso del gradiente (Gradient Descent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "\n",
    "X = np.random.rand(3,2)\n",
    "Y = np.array([[0, 1]])\n",
    "\n",
    "m = X.shape[1]\n",
    "\n",
    "W = np.array([[0.1], [-0.1], [0.01]])\n",
    "b = 0.1\n",
    "\n",
    "print(\"m: \", m)\n",
    "print(\"W inicial: \",W)\n",
    "print(\"b inicial: \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "for i in range(2000): #2000 iteraciones del descenso del gradiente\n",
    "    #1. Propagacion hacia adelante\n",
    "    Z = linear_activation(W,b,X)\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    #2. Descenso del gradiente\n",
    "    dz = A - Y\n",
    "    dW = np.dot(X,dz.T) / m\n",
    "    db = np.sum(dz) / m\n",
    "    \n",
    "    #Actualizacion de los parametros W y b\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    #Estimacion del costo actual. A mayor numero de iteraciones menor costo final.\n",
    "    J = cost(loss(Y,A))\n",
    "    \n",
    "    if(i%100 == 0):\n",
    "        print(\"costo: \", J)\n",
    "\n",
    "print(\"W actualizado: \",W)\n",
    "print(\"b actualizado: \",b)\n",
    "print(\"costo total: \", J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predicción\n",
    "\n",
    "La predicción consiste en aplicar forward propagation utilizando los W y b optimizados mediante descenso del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(W,b,X):\n",
    "    z = linear_activation(W,b,X)\n",
    "    A = sigmoid(z)\n",
    "    return np.round(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = predict(W,b,X)\n",
    "print(\"predicciones: \",np.round(Y_hat))\n",
    "print(\"clases originales: \", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Regresión Logística sobre un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import generate_data, visualize, plot_decision_boundary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generate_data('blobs')\n",
    "Y = Y.reshape(1,len(Y))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color= ['red' if y == 1 else 'green' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1. inicilicemos parametros W y b\n",
    "m = X.shape[1]\n",
    "\n",
    "W = np.random.randn(X.shape[0],1)\n",
    "b = 0\n",
    "\n",
    "print(\"m: \", m)\n",
    "print(\"W inicial: \",W)\n",
    "print(\"b inicial: \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2. Regresion logistica mediante descenso del gradiente\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(10000): #1000 iteraciones del descenso del gradiente\n",
    "    Z = linear_activation(W,b,X)\n",
    "    A = sigmoid(Z)\n",
    "    dz = A - Y\n",
    "    dW = np.dot(X,dz.T) / m\n",
    "    db = np.sum(dz) / m\n",
    "    J = np.sum(-(Y * np.log(A) + (1-Y)*np.log(1-A))) / m\n",
    "    \n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print(\"costo: \", J)\n",
    "\n",
    "print(\"W actualizado: \",W)\n",
    "print(\"b actualizado: \",b)\n",
    "print(\"costo final (error), despues de \",i+1,\" iteraciones: \", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(predict(W,b,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_lr(W, b, X, y):\n",
    "    X = X.T\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    #Z = pred_func(W,b,np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = predict(W,b,np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_lr(W, b, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
